{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMz7LZ0Mvw3z"
      },
      "source": [
        "# Handling text exercise 1\n",
        "Credits to Giovanni Colavizza for the material https://www.giovannicolavizza.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0ABxczhvw3z"
      },
      "source": [
        "## Welcome! This exercise consists of:\n",
        "\n",
        "## Part 1: Hands on tutorial\n",
        "1. Implementing the natural language processing pipeline\n",
        "2. Solving two typical language processing tasks:\n",
        "   * Topic detection\n",
        "   * Semantic analysis\n",
        "\n",
        "\n",
        "### We will be working with classic books in plain text:\n",
        "\n",
        "#### To run this notebook, you need to install the following libraries:\n",
        "\n",
        "conda install nltk gensim spacy <br>\n",
        "pip install pyLDAvis  <br>\n",
        "pip install vaderSentiment  <br>\n",
        "pip install empath <br>\n",
        "python -m spacy download en  <br>\n",
        "python -m nltk.downloader punkt <br>\n",
        "python -m nltk.downloader all-corpora <br>\n",
        "\n",
        "## Part 2: Handling text comprehension questions\n",
        "- You can test your understanding of important NLP concepts. Solutions will be released next week."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNMJPDFevw30"
      },
      "source": [
        "## Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpvwXFKVvw30",
        "outputId": "9e848812-667c-492d-a8a0-5116f66dcfda"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "import os, codecs, string, random\n",
        "import numpy as np\n",
        "from numpy.random import seed as random_seed\n",
        "from numpy.random import shuffle as random_shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "#NLP libraries\n",
        "import spacy, nltk, gensim, sklearn\n",
        "import pyLDAvis.gensim_models\n",
        "\n",
        "#Vader\n",
        "import vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "#Scikit imports\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFQDlcE-vw30"
      },
      "source": [
        "#### In this tutorial we will be working with text in English. The same principles apply, but the methods cannot be applied directly to the text in unknown or other languages. We will be working mainly with the Spacy library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6tJ3meCvw30"
      },
      "source": [
        "Initialize the Spacy analyzer in English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "di0jp49Kvw30"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_bBI4oJvw30"
      },
      "source": [
        "Load the books"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2MB2qgCvw30",
        "outputId": "b1b0664c-7e53-4aa9-bb90-1a941d51ca65"
      },
      "outputs": [],
      "source": [
        "# Download the Gutenberg dataset\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "# book filenames\n",
        "book_files = gutenberg.fileids()\n",
        "\n",
        "books = [gutenberg.raw(file) for file in book_files]\n",
        "\n",
        "for file, book in zip(book_files, books):\n",
        "    print(f\"Book: {file} | Characters: {len(book)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qesFA2vivw31"
      },
      "source": [
        "Print the beginning of Julius Caesar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17WkdKw0vw31",
        "outputId": "a34eb749-40cc-4770-f67c-6db11af8f5c8"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(books[14][0:600])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXDI89bpvw31"
      },
      "source": [
        "#### Let's remove the new lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "ValAeruevw31"
      },
      "outputs": [],
      "source": [
        "books = [\" \".join(b.split()) for b in books]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SgT7Kvuvw31",
        "outputId": "306f8355-1030-4dcc-91d7-170f9a370774"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[The Tragedie of Julius Caesar by William Shakespeare 1599] Actus Primus. Scoena Prima. Enter Flauius, Murellus, and certaine Commoners ouer the Stage. Flauius. Hence: home you idle Creatures, get you home: Is this a Holiday? What, know you not (Being Mechanicall) you ought not walke Vpon a labouring day, without the signe Of your Profession? Speake, what Trade art thou? Car. Why Sir, a Carpenter Mur. Where is thy Leather Apron, and thy Rule? What dost thou with thy best Apparrell on? You sir, what Trade are you? Cobl. Truely Sir, in respect of a fine Workman, I am but as you would say, a Cobl\n"
          ]
        }
      ],
      "source": [
        "print(books[14][0:600])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49jYdf2-vw31"
      },
      "source": [
        "[link text](https://)### Load Hamlet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIV1jR0Pvw31"
      },
      "outputs": [],
      "source": [
        "book = books[15]\n",
        "\n",
        "#put in raw text, get a Spacy object\n",
        "doc = nlp(book)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmEMOd3Nvw31"
      },
      "source": [
        "## Let's create our own NLP pipeline with Spacy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCBNQv-Pvw31"
      },
      "source": [
        "### Step 1: Sentence splitting\n",
        "\n",
        "#### May sound trivial, but it's not! (e.g., U.K. or Yahoo!) Spacy uses a statistical model to generate accurate predictions. This works well out of the box for general-purpose text or web text. For social media use dedicated libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_XvA2XGvw31",
        "outputId": "fb64f965-bea3-4e33-eb6a-63218d1b85fb"
      },
      "outputs": [],
      "source": [
        "sentences = [sent for sent in doc.sents]\n",
        "print('Sentence 1:',sentences[0],'\\n')\n",
        "print('Sentence 2:',sentences[1],'\\n')\n",
        "print('Sentence 3:',sentences[2],'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRp3fa7Cvw31"
      },
      "source": [
        "### Step 2: Tokenization\n",
        "\n",
        "#### The task of splitting a text into meaningful segments called tokens. We segment the sentence into words, punctuation, etc. This is done by first splitting on whitespace characters and them applying rules specific to each language. For example, \"don't\" does not contain whitespace, but should be split into two tokens, \"do\" and \"n't\", while \"U.K.\" should always remain one token and \"but,\" shouldn't."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HVsX02Evw31"
      },
      "source": [
        "### We will analyze the methods on an example sentence first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNfsEh_ovw31",
        "outputId": "4fe9ff7d-5ac8-4bb8-c008-178c703dc48e"
      },
      "outputs": [],
      "source": [
        "example = 'I am already far north of London, and as I walk in the streets of Petersburgh, I feel a cold northern breeze play upon my cheeks, which braces my nerves and fills me with delight.'\n",
        "\n",
        "doc = nlp(example)\n",
        "\n",
        "#strings are encoded to hashes\n",
        "tokens = [token.text for token in doc]\n",
        "\n",
        "print(example,'\\n')\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKRHx31Ivw31"
      },
      "source": [
        "### Step 3: Part of speech tagging\n",
        "\n",
        "#### The model makes a prediction of which tag or label most likely applies in this context. For example, a word following \"the\" in English is most likely a noun."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJPyy-Wevw31",
        "outputId": "263ff888-564e-4953-edcc-212bacd077f1"
      },
      "outputs": [],
      "source": [
        "pos_tagged = [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "print(example,'\\n')\n",
        "print(pos_tagged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edUKbHeOvw31",
        "outputId": "f877d513-c903-41ee-aefe-f91ee815f511"
      },
      "outputs": [],
      "source": [
        "print(spacy.explain('CCONJ'))\n",
        "print(spacy.explain('ADP'))\n",
        "print(spacy.explain('DET'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f90neqbAvw31"
      },
      "source": [
        "### When unsure, see here for more details: http://universaldependencies.org/u/pos/all.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZCUmmDNvw31"
      },
      "source": [
        "#### More detailed annotation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5RFM7hUvw31",
        "outputId": "5e95a0fc-3d39-483f-e62a-10ef87834809"
      },
      "outputs": [],
      "source": [
        "pos_tagged = [(token.text, token.tag_) for token in doc]\n",
        "\n",
        "print(pos_tagged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV1c11xSvw32",
        "outputId": "ff5b2cb7-8d58-42b4-fd48-83e786abf737"
      },
      "outputs": [],
      "source": [
        "print(spacy.explain('PRP'))\n",
        "print(spacy.explain('VBP'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHOW5uCRvw32"
      },
      "source": [
        "### Step 4: Named entity recognition\n",
        "\n",
        "#### For example, a person, a country, a product or a book title. Spacy can recognise various types of named entities in a document. This doesn't always work perfectly and might need some tuning later, depending on your use case.\n",
        "\n",
        "Built in entity types: <br>\n",
        "\n",
        "PERSON  People, including fictional.  <br>\n",
        "NORP\tNationalities or religious or political groups. <br>\n",
        "FAC\tBuildings, airports, highways, bridges, etc. <br>\n",
        "ORG\tCompanies, agencies, institutions, etc. <br>\n",
        "GPE\tCountries, cities, states. <br>\n",
        "LOC\tNon-GPE locations, mountain ranges, bodies of water. <br>\n",
        "PRODUCT\tObjects, vehicles, foods, etc. (Not services.) <br>\n",
        "EVENT\tNamed hurricanes, battles, wars, sports events, etc. <br>\n",
        "WORK_OF_ART\tTitles of books, songs, etc. <br>\n",
        "LAW\tNamed documents made into laws. <br>\n",
        "LANGUAGE\tAny named language. <br>\n",
        "DATE\tAbsolute or relative dates or periods. <br>\n",
        "TIME\tTimes smaller than a day. <br>\n",
        "PERCENT\tPercentage, including \"%\". <br>\n",
        "MONEY\tMonetary values, including unit. <br>\n",
        "QUANTITY\tMeasurements, as of weight or distance. <br>\n",
        "ORDINAL\t\"first\", \"second\", etc. <br>\n",
        "CARDINAL\tNumerals that do not fall under another type. <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLhR04v_vw32",
        "outputId": "11424405-8842-400f-e003-802faace9dcb"
      },
      "outputs": [],
      "source": [
        "print(example,'\\n')\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYtPWk8avw32"
      },
      "source": [
        "### Step 5: Removing stop words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wSRq98Evw32"
      },
      "source": [
        "Let's check out all the stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo2xk9Gxvw32",
        "outputId": "0002c725-674d-4318-f302-a8e0fa38f845"
      },
      "outputs": [],
      "source": [
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "print('Number of stop words: %d' % len(spacy_stopwords))\n",
        "print('First ten stop words:',list(spacy_stopwords)[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6Mhqijgvw32"
      },
      "source": [
        "Detecting stopwords in the example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhHMVPOZvw32",
        "outputId": "9222426e-2047-4b24-bd7b-30d5b9e9cdec"
      },
      "outputs": [],
      "source": [
        "print(example,'\\n')\n",
        "\n",
        "stop_words = [token.text for token in doc if token.is_stop]\n",
        "\n",
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dquv4MZbvw32"
      },
      "source": [
        "#### Sometimes it's useful to define a custom list of stopwords. Other solutions: https://www.ranks.nl/stopwords\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uXx2t98vw32"
      },
      "source": [
        "### Step 6: Lemmatization\n",
        "\n",
        "#### A rule-based deterministic lemmatizer maps the surface form of a token to a lemma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjW6ehukvw32",
        "outputId": "6fb37560-2ea1-4b51-858c-529986b67596"
      },
      "outputs": [],
      "source": [
        "print(example,'\\n')\n",
        "for token in doc:\n",
        "    if token.text != token.lemma_:\n",
        "        print(token.text,'--->',token.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTFwVl2yvw39"
      },
      "source": [
        "### Step 7: Chunking (shallow parsing)\n",
        "\n",
        "#### Noun chunks are \"base noun phrases\" – flat phrases that have a noun as their head -- a noun plus the words describing the noun – for example, \"the lavish green grass\" or \"the world’s largest tech fund\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uECnLU3uvw39",
        "outputId": "398135f5-1ae6-449e-eeee-ad4ae946ed07"
      },
      "outputs": [],
      "source": [
        "print(example,'\\n')\n",
        "\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugBsPlvGvw39"
      },
      "source": [
        "### Step 8: Dependancy parsing\n",
        "\n",
        "#### The terms head and child describe the words connected by an arc in the dependency tree. The type of syntactic relation that connects the child to the head can be obtain through .dep_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WJVX1gXvw39",
        "outputId": "c15daf3b-7cec-4fd4-dc6f-0cd6c3b8be0e"
      },
      "outputs": [],
      "source": [
        "print(example,'\\n')\n",
        "\n",
        "for token in doc:\n",
        "    print('Token:',token.text,'Head:',token.head.text, 'Children:',[child for child in token.children])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptfDxxvnvw39"
      },
      "source": [
        "### Counting word occurences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A47ePeLxvw39",
        "outputId": "ee894861-6c98-4e58-f1a6-111d2a78b304"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(example,'\\n')\n",
        "words = [token.text for token in doc]\n",
        "\n",
        "# five most common tokens\n",
        "word_freq = Counter(words)\n",
        "common_words = word_freq.most_common()\n",
        "\n",
        "print(common_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6ZdDvkevw39"
      },
      "source": [
        "### Without stop words and punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RNZiVHbvw39",
        "outputId": "81c74a46-5795-44ff-f70e-8db0d123f86d"
      },
      "outputs": [],
      "source": [
        "words = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
        "\n",
        "# five most common tokens\n",
        "word_freq = Counter(words)\n",
        "common_words = word_freq.most_common()\n",
        "\n",
        "print(common_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D6pDgNbvw39"
      },
      "source": [
        "## Putting all the components together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpNzb37-vw39"
      },
      "source": [
        "### Under the hood: the pipeline [\"tokenizer\",\"tagger\", \"parser\", \"ner\"]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXd5sD6Vvw39"
      },
      "source": [
        "#### The NLP pipeline with Spacy\n",
        "<img src=\"spacy.png\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dpOMrvA4vw39"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXSKXFXFvw39"
      },
      "source": [
        "### If you don't need a particular component of the pipeline – for example, the tagger or the parser, you can disable loading it. This can sometimes make a big difference and improve loading speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2NOcWDPvw39",
        "outputId": "91436b5b-dae0-43e4-974e-1b3f313dc315"
      },
      "outputs": [],
      "source": [
        "nlp.remove_pipe('parser')\n",
        "nlp.remove_pipe('tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8G6-t-Yvw3_"
      },
      "source": [
        "# Task 1: Topic detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V702idbq6Zn0",
        "outputId": "b95b4b65-c45b-4a6d-d554-f38180898379"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Let's load our corpus via NLTK this time\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "\n",
        "\n",
        "corpus_root = \"gutenberg_books\"\n",
        "os.makedirs(corpus_root, exist_ok=True)\n",
        "\n",
        "for file in gutenberg.fileids():\n",
        "    text = gutenberg.raw(file)\n",
        "    with open(os.path.join(corpus_root, file), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "\n",
        "print(\"Gutenberg books saved in:\", corpus_root)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKh7SmVo6eFF"
      },
      "outputs": [],
      "source": [
        "\n",
        "our_books = PlaintextCorpusReader(corpus_root, '.*.txt')\n",
        "\n",
        "print(\"Available books:\", our_books.fileids())\n",
        "\n",
        "book_content = our_books.raw(\"shakespeare-caesar.txt\")\n",
        "print(\"\\nSample from Macbeth:\\n\", book_content[:500])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TF-IDF computes the weight of each word in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "documents = [our_books.raw(fileid) for fileid in our_books.fileids()]\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)  # Keep most important words\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# Get words and their TF-IDF scores\n",
        "word_scores = dict(zip(vectorizer.get_feature_names_out(), tfidf_matrix.sum(axis=0).A1))\n",
        "\n",
        "# Generate word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_scores)\n",
        "\n",
        "# Display\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"TF-IDF Word Cloud\")\n",
        "plt.show()\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "#Get the sum of TF-IDF scores for each word across all documents\n",
        "tfidf_sum = np.sum(tfidf_matrix, axis=0).A1  # Flatten the matrix\n",
        "\n",
        "top_indices = np.argsort(tfidf_sum)[-20:][::-1]  # Get indices of the top 20, in descending order\n",
        "\n",
        "print(\"Top 20 Words with TF-IDF Scores:\")\n",
        "for i in top_indices:\n",
        "    print(f\"{feature_names[i]}: {tfidf_sum[i]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDpDbeuzvw3_",
        "outputId": "d9849d30-82d7-44e1-f223-c27d19855624"
      },
      "outputs": [],
      "source": [
        "# Get the chunks again (into smaller chunks)\n",
        "nltk.download('punkt_tab')\n",
        "def get_chunks(l, n):\n",
        "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
        "    for i in range(0, len(l), n):\n",
        "        yield l[i:i + n]\n",
        "\n",
        "\n",
        "book_id = {f:n for n,f in enumerate(our_books.fileids())} # dictionary of books\n",
        "chunks = list()\n",
        "chunk_class = list() # this list contains the original book of the chunk, for evaluation\n",
        "\n",
        "limit = 60 # how many chunks total\n",
        "size = 50 # how many sentences per chunk/page\n",
        "\n",
        "for f in our_books.fileids():\n",
        "    sentences = our_books.sents(f)\n",
        "    print(f)\n",
        "    print('Number of sentences:',len(sentences))\n",
        "\n",
        "    # create chunks\n",
        "    chunks_of_sents = [x for x in get_chunks(sentences,size)] # this is a list of lists of sentences, which are a list of tokens\n",
        "    chs = list()\n",
        "\n",
        "    # regroup so to have a list of chunks which are strings\n",
        "    for c in chunks_of_sents:\n",
        "        grouped_chunk = list()\n",
        "        for s in c:\n",
        "            grouped_chunk.extend(s)\n",
        "        chs.append(\" \".join(grouped_chunk))\n",
        "    print(\"Number of chunks:\",len(chs),'\\n')\n",
        "\n",
        "    # filter to the limit, to have the same number of chunks per book\n",
        "    chunks.extend(chs[:limit])\n",
        "    chunk_class.extend([book_id[f] for _ in range(len(chs[:limit]))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "6bxjdIyzvw3_"
      },
      "outputs": [],
      "source": [
        "STOPWORDS = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "processed_docs = list()\n",
        "tdidf_df = list()\n",
        "for doc in nlp.pipe(chunks, n_process=5, batch_size=10):\n",
        "\n",
        "    # Process document using Spacy NLP pipeline.\n",
        "    ents = doc.ents  # Named entities\n",
        "\n",
        "    # Keep only words (no numbers, no punctuation).\n",
        "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
        "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
        "\n",
        "    # Remove common words from a stopword list and keep only words of length 3 or more.\n",
        "    doc = [token for token in doc if token not in STOPWORDS and len(token) > 2]\n",
        "\n",
        "    # Add named entities, but only if they are a compound of more than word.\n",
        "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
        "\n",
        "    processed_docs.append(doc)\n",
        "\n",
        "docs = processed_docs\n",
        "\n",
        "del processed_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "tWIr7fXwvw3_"
      },
      "outputs": [],
      "source": [
        "# Add bigrams too\n",
        "from gensim.models.phrases import Phrases\n",
        "\n",
        "# Add bigrams to docs (only ones that appear 15 times or more).\n",
        "bigram = Phrases(docs, min_count=15)\n",
        "\n",
        "for idx in range(len(docs)):\n",
        "    for token in bigram[docs[idx]]:\n",
        "        if '_' in token:\n",
        "            # Token is a bigram, add to document.\n",
        "            docs[idx].append(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAwQ7g6Ovw3_",
        "outputId": "c5872d70-5a4a-4a0a-cf2c-dde2ef9ef1a5"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary representation of the documents, and filter out frequent and rare words.\n",
        "from gensim.corpora import Dictionary\n",
        "dictionary = Dictionary(docs)\n",
        "\n",
        "# Remove rare and common tokens.\n",
        "# Filter out words that occur too frequently or too rarely.\n",
        "max_freq = 0.5\n",
        "min_wordcount = 5\n",
        "dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
        "\n",
        "# Bag-of-words representation of the documents.\n",
        "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
        "#MmCorpus.serialize(\"models/corpus.mm\", corpus)\n",
        "\n",
        "print('Number of unique tokens: %d' % len(dictionary))\n",
        "print('Number of chunks: %d' % len(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "pdAMZNgLvw3_"
      },
      "outputs": [],
      "source": [
        "# models\n",
        "from gensim.models import LdaMulticore\n",
        "params = {'passes': 10, 'random_state': seed}\n",
        "base_models = dict()\n",
        "model = LdaMulticore(corpus=corpus, num_topics=4, id2word=dictionary, workers=6,\n",
        "                passes=params['passes'], random_state=params['random_state'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bG2Uegovw3_",
        "outputId": "bf8d1e05-e207-4cc8-808d-c76c549b3484"
      },
      "outputs": [],
      "source": [
        "model.show_topics(num_words=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IK2rMeGqvw3_",
        "outputId": "a821fe4d-892e-42c4-a812-ec598b175ac0"
      },
      "outputs": [],
      "source": [
        "model.show_topic(1,20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SysuL_Q3vw3_",
        "outputId": "77a474bc-9970-4165-bf39-1a182b347f80"
      },
      "outputs": [],
      "source": [
        "sorted(model[corpus[0]],key=lambda x:x[1],reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j91Juep3vw3_",
        "outputId": "4b9accb9-bb24-4883-be86-9e4e0b7d523f"
      },
      "outputs": [],
      "source": [
        "# plot topics\n",
        "data =  pyLDAvis.gensim_models.prepare(model, corpus, dictionary)\n",
        "pyLDAvis.display(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "B2j5PKRuvw3_"
      },
      "outputs": [],
      "source": [
        "# assignment\n",
        "sent_to_cluster = list()\n",
        "for n,doc in enumerate(corpus):\n",
        "    if doc:\n",
        "        cluster = max(model[doc],key=lambda x:x[1])\n",
        "        sent_to_cluster.append(cluster[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z52meG4Uvw3_",
        "outputId": "fa3ce7e4-2ee6-425d-b458-73b39fd0a0d1"
      },
      "outputs": [],
      "source": [
        "# accuracy\n",
        "relevant_books = ['shakespeare-caesar.txt','chesterton-thursday.txt','austen-persuasion.txt', 'melville-moby_dick.txt']\n",
        "\n",
        "from collections import Counter\n",
        "for book, cluster in book_id.items():\n",
        "    assignments = list()\n",
        "    for real,given in zip(chunk_class,sent_to_cluster):\n",
        "        if real == cluster:\n",
        "            assignments.append(given)\n",
        "    most_common,num_most_common = Counter(assignments).most_common(1)[0] # 4, 6 times\n",
        "    if book in relevant_books:\n",
        "        print(book,\":\",most_common,\"-\",num_most_common)\n",
        "        print(\"Accuracy:\",num_most_common/limit)\n",
        "        print(\"------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw8TpCSovw3_"
      },
      "source": [
        "# Task 4: Semantic analysis based on lexical categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "vAdkkfgPvw3_"
      },
      "outputs": [],
      "source": [
        "from empath import Empath\n",
        "lexicon = Empath()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLKJISI3vw3_"
      },
      "source": [
        "### Let's see what pre-build categories we can study! More information: https://hci.stanford.edu/publications/2016/ethan/empath-chi-2016.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gPCaV5fvw3_",
        "outputId": "1201f87c-1f47-4401-9e1b-43d963f5a87b"
      },
      "outputs": [],
      "source": [
        "for cat in list(lexicon.cats.keys())[:15]:\n",
        "    print(cat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjI4s3TOvw3_"
      },
      "source": [
        "### For each category, we can examine representative terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpM5gNaCvw3_",
        "outputId": "8c9ce8c2-5f1f-4e14-9661-d7e74f307acc"
      },
      "outputs": [],
      "source": [
        "lexicon.cats[\"health\"][:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2Pz-I_Pvw3_"
      },
      "source": [
        "### Studying 'Persuasion'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "qamVG2UXvw3_"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(books[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "luT9ltUBvw4A"
      },
      "outputs": [],
      "source": [
        "empath_features = lexicon.analyze(doc.text,categories = [\"disappointment\", \"pain\", \"joy\", \"beauty\", \"affection\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwAYVYo_vw4A",
        "outputId": "bc6bc5b2-880b-4f71-d46a-69e1082dff25"
      },
      "outputs": [],
      "source": [
        "empath_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjGRtZy3vw4A"
      },
      "outputs": [],
      "source": [
        "\n",
        "empath_features = lexicon.analyze(doc.text,categories = [\"disappointment\", \"pain\", \"joy\", \"beauty\", \"affection\"], normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lZHSZNHvw4A",
        "outputId": "bd79e421-afc7-4f9c-9417-75e25ee898bb"
      },
      "outputs": [],
      "source": [
        "empath_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWhqhRxMvw4A"
      },
      "source": [
        "### Usecase: the evolution of topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "qSacpxyxvw4A"
      },
      "outputs": [],
      "source": [
        "bins = range(0,len(doc.text),150000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "HND0tACQvw4A"
      },
      "outputs": [],
      "source": [
        "love = []\n",
        "pain = []\n",
        "beauty = []\n",
        "affection = []\n",
        "\n",
        "\n",
        "for cnt,i in enumerate(bins[:-1]):\n",
        "    empath_features = lexicon.analyze(doc.text[bins[cnt]:bins[cnt+1]],\n",
        "                                      categories = [\"love\", \"pain\", \"joy\", \"beauty\", \"affection\"], normalize = True)\n",
        "    love.append(empath_features[\"love\"])\n",
        "    pain.append(empath_features[\"pain\"])\n",
        "    beauty.append(empath_features[\"beauty\"])\n",
        "    affection.append(empath_features[\"affection\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "nXIMP_vqvw4A",
        "outputId": "e8266cd9-13a7-4b71-dcd7-afdf042f45a6"
      },
      "outputs": [],
      "source": [
        "plt.plot(love,label = \"love\")\n",
        "plt.plot(beauty, label = \"beauty\")\n",
        "plt.plot(affection, label = \"affection\")\n",
        "plt.plot(pain,label = \"pain\")\n",
        "\n",
        "plt.xlabel(\"progression in the book\")\n",
        "plt.ylabel(\"frequency of a category\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can observe how as the story progresses, beauty categories decreases, while love, affection and pain intensify!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ-TEKcSvw4A"
      },
      "source": [
        "### We can create custom categories based on seed terms!\n",
        "### Models trained on: fiction, nytimes or reddit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAXGpM3mvw4A",
        "outputId": "9e19a6bd-8bda-440d-e9cb-3f44e75df3b8"
      },
      "outputs": [],
      "source": [
        "lexicon.create_category(\"healthy_food\", [\"healthy_food\",\"low_carb\",\"kale\",\"avocado\"], model=\"nytimes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhVF8Sa7vw4A",
        "outputId": "215689f1-6a43-4bc2-a230-eb2ff1778a3e"
      },
      "outputs": [],
      "source": [
        "lexicon.create_category(\"healthy_food\", [\"healthy_food\",\"low_carb\",\"kale\",\"avocado\"], model=\"reddit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCl628n9vw4A",
        "outputId": "5f0df5ac-8986-4c7d-8b3c-3a6542678cd5"
      },
      "outputs": [],
      "source": [
        "lexicon.create_category(\"festive_food\", [\"festive_food\",\"turkey\",\"eggnog\"], model=\"nytimes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLtmc5MZvw4A"
      },
      "source": [
        "### List of further resources\n",
        "    * NLTK\n",
        "    * Jellyfish: string similarity library\n",
        "    * TextBlob: simplified text processing\n",
        "    * PyEnchant: language detection\n",
        "    * WordNet: ontology of concepts (inc. in NLTK)\n",
        "    * Stanford NLP (Java)\n",
        "    * Tweet NLP: https://www.cs.cmu.edu/~ark/TweetNLP/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otmvkxjMvw4A"
      },
      "source": [
        "## Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLb7e_3yvw4A"
      },
      "source": [
        "### Question 1:\n",
        "An NLP preprocessing model, using the list of stopwords ['a', 'the', 'is', 'you', 'I','my','with','was'] for the input sentence\n",
        "“I was driving down the street with my CAR! :)” <br>\n",
        "gives the following output: {driving, down, street, car, :)}.\n",
        "The model consists of: <br>\n",
        "\n",
        "a) Stopword Removal, Casefolding and Stemming <br>\n",
        "b) Stopword Removal and Casefolding <br>\n",
        "c) Stopword Removal and Stemming <br>\n",
        "d) Casefolding and Stemming <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZBAht6Dvw4A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmEWntgCvw4A"
      },
      "source": [
        "### Question 2:\n",
        "What statement is false about topic detection using LSA: <br>\n",
        "\n",
        "a) It finds the representation of documents and words in the latent \"topic space\" <br>\n",
        "b) It produces topics interpretable in “word space” <br>\n",
        "c) A document's topic representation is a probability distribution over topics <br>\n",
        "d) The vectors representing topics in “word space” are linearly independent <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxHme-UBvw4A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmrB-jPBvw4A"
      },
      "source": [
        "### Question 3:\n",
        "The problem of having more features than documents when using a TF-IDF matrix for document classification cannot be addressed by: <br>\n",
        "\n",
        "a) Using SVD for dimensionality reduction <br>\n",
        "b) Using min-max scaling of features <br>\n",
        "c) Penalizing very large positive and very large negative weights (a.k.a. regularization) <br>\n",
        "d) Using online feature selection <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZK-jeSTvw4A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYfINOH9vw4A"
      },
      "source": [
        "### Question 4:\n",
        "Which of the following is false about word vectors: <br>\n",
        "\n",
        "a) Word2vec is used to achieve a lower dimensional representation than bag of words <br>\n",
        "b) Semantically similar words typically have similar word2vec vectors <br>\n",
        "c) Each dimension in the word2vec embedding has a clear interpretation <br>\n",
        "d) Word2Vec can be used for the computation of sentence vectors <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7nJ61_kvw4A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQRaU38Gvw4A"
      },
      "source": [
        "### Question 5:\n",
        "Given the following word (W) / context (C) co-occurrence frequencies, which statement is true regarding the pointwise mutual information (PMI)?\n",
        "\n",
        "|  |W1|W2|W3|\n",
        "|--|--|--|--|\n",
        "|C1| 2| 0| 3|\n",
        "|C2| 6| 4| 0|\n",
        "|C3| 1| 1| 3|\n",
        "\n",
        "a) PMI(w=W1, c=C2) > PMI(w=W2, c=C2) > PMI(w=W3, c=C3) <br>\n",
        "b) PMI(w=W2, c=C2) > PMI(w=W1, c=C2) >  PMI(w=W3, c=C3) <br>\n",
        "c) PMI(w=W3, c=C3) > PMI(w=W1, c=C2) > PMI(w=W2, c=C2) <br>\n",
        "d) PMI(w=W3, c=C3) > PMI(w=W2, c=C2) > PMI(w=W1, c=C2) <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYTz6ok7vw4B",
        "tags": []
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "228px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
